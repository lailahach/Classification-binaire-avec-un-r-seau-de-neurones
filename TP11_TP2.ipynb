{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMa2P+PrK5D96hD9Z4qdotq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lailahach/Classification-binaire-avec-un-r-seau-de-neurones/blob/main/TP11_TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuJ6Bby2FWHd",
        "outputId": "a1f57147-622c-47ea-c275-0c4526555022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perte initiale: 0.31446683406829834\n",
            "Perte finale approx.: 0.0004981525707989931\n",
            "Probabilités: [0.0242 0.9794 0.9765 0.0207]\n",
            "Prédictions binaires: [0. 1. 1. 0.]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# MLP XOR en TensorFlow\n",
        "# =========================\n",
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Reproductibilité\n",
        "seed = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# Données XOR\n",
        "X = np.array([[0.,0.],\n",
        "              [0.,1.],\n",
        "              [1.,0.],\n",
        "              [1.,1.]], dtype=np.float32)\n",
        "y = np.array([[0.],\n",
        "              [1.],\n",
        "              [1.],\n",
        "              [0.]], dtype=np.float32)\n",
        "\n",
        "# Modèle 2-4-1 avec sigmoïde\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(4, activation=\"sigmoid\", input_shape=(2,)),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "# --- Assertions structure (dimensions des poids) ---\n",
        "# Dense #1 : kernel (in_features=2, units=4), bias (4,)\n",
        "k1, b1 = model.layers[0].weights\n",
        "assert tuple(k1.shape) == (2, 4), f\"Poids couche cachée: attendu (2,4) obtenu {k1.shape}\"\n",
        "assert tuple(b1.shape) == (4,),   f\"Biais couche cachée: attendu (4,) obtenu {b1.shape}\"\n",
        "\n",
        "# Dense #2 : kernel (in_features=4, units=1), bias (1,)\n",
        "k2, b2 = model.layers[1].weights\n",
        "assert tuple(k2.shape) == (4, 1), f\"Poids couche sortie: attendu (4,1) obtenu {k2.shape}\"\n",
        "assert tuple(b2.shape) == (1,),   f\"Biais couche sortie: attendu (1,) obtenu {b2.shape}\"\n",
        "\n",
        "# Optimiseur & perte (MSE)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1.0)  # XOR converge vite avec LR élevé\n",
        "loss_fn   = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Boucle d'entraînement manuelle (rétroprop + descente de gradient)\n",
        "epochs = 5000\n",
        "loss_history = []\n",
        "\n",
        "# Perte initiale (pour assertion d'amélioration)\n",
        "with tf.GradientTape() as tape:\n",
        "    y_hat0 = model(X, training=True)\n",
        "    loss0 = loss_fn(y, y_hat0)\n",
        "loss_history.append(float(loss0.numpy()))\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_pred = model(X, training=True)\n",
        "        loss = loss_fn(y, y_pred)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    if epoch % 500 == 0:\n",
        "        loss_history.append(float(loss.numpy()))\n",
        "\n",
        "# Évaluations\n",
        "y_prob = model(X, training=False).numpy()\n",
        "y_pred_bin = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "# --- Assertions comportement ---\n",
        "# 1) Sorties dans [0,1] (sigmoïde)\n",
        "assert np.all(y_prob >= 0.0) and np.all(y_prob <= 1.0), \"Les probabilités doivent être dans [0,1].\"\n",
        "\n",
        "# 2) Amélioration de la perte\n",
        "assert loss_history[-1] < loss_history[0], f\"La perte n'a pas diminué: {loss_history[0]:.4f} -> {loss_history[-1]:.4f}\"\n",
        "\n",
        "# 3) Forme de sortie (4,1)\n",
        "assert y_prob.shape == (4,1), f\"Sortie attendue (4,1), obtenu {y_prob.shape}\"\n",
        "\n",
        "print(\"Perte initiale:\", loss_history[0])\n",
        "print(\"Perte finale approx.:\", loss_history[-1])\n",
        "print(\"Probabilités:\", np.round(y_prob, 4).reshape(-1))\n",
        "print(\"Prédictions binaires:\", y_pred_bin.reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# MLP XOR en PyTorch\n",
        "# =========================\n",
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Reproductibilité\n",
        "seed = 42\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Données XOR\n",
        "X = torch.tensor([[0.,0.],\n",
        "                  [0.,1.],\n",
        "                  [1.,0.],\n",
        "                  [1.,1.]], dtype=torch.float32, device=device)\n",
        "y = torch.tensor([[0.],\n",
        "                  [1.],\n",
        "                  [1.],\n",
        "                  [0.]], dtype=torch.float32, device=device)\n",
        "\n",
        "# Modèle 2-4-1 avec sigmoïde\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 4),   # (out=4, in=2)\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(4, 1),   # (out=1, in=4)\n",
        "    nn.Sigmoid()\n",
        ").to(device)\n",
        "\n",
        "# --- Assertions structure (dimensions des poids) ---\n",
        "l1 = model[0]\n",
        "l2 = model[2]\n",
        "# nn.Linear.weight a la forme (out_features, in_features)\n",
        "assert tuple(l1.weight.shape) == (4, 2), f\"Poids couche cachée: attendu (4,2) obtenu {tuple(l1.weight.shape)}\"\n",
        "assert tuple(l1.bias.shape)   == (4,),   f\"Biais couche cachée: attendu (4,) obtenu {tuple(l1.bias.shape)}\"\n",
        "assert tuple(l2.weight.shape) == (1, 4), f\"Poids couche sortie: attendu (1,4) obtenu {tuple(l2.weight.shape)}\"\n",
        "assert tuple(l2.bias.shape)   == (1,),   f\"Biais couche sortie: attendu (1,) obtenu {tuple(l2.bias.shape)}\"\n",
        "\n",
        "# Perte & optimiseur\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=1.0)\n",
        "\n",
        "# Perte initiale\n",
        "with torch.no_grad():\n",
        "    y_hat0 = model(X)\n",
        "    loss0 = criterion(y_hat0, y).item()\n",
        "\n",
        "epochs = 5000\n",
        "loss_last = loss0\n",
        "for epoch in range(1, epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X)\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 500 == 0:\n",
        "        loss_last = loss.item()\n",
        "\n",
        "# Évaluations\n",
        "with torch.no_grad():\n",
        "    y_prob = model(X).cpu().numpy()\n",
        "    y_bin  = (y_prob >= 0.5).astype(np.float32)\n",
        "\n",
        "# --- Assertions comportement ---\n",
        "# 1) Sorties dans [0,1]\n",
        "assert np.all(y_prob >= 0.0) and np.all(y_prob <= 1.0), \"Les probabilités doivent être dans [0,1].\"\n",
        "\n",
        "# 2) Amélioration de la perte\n",
        "assert loss_last < loss0, f\"La perte n'a pas diminué: {loss0:.4f} -> {loss_last:.4f}\"\n",
        "\n",
        "# 3) Forme de sortie (4,1)\n",
        "assert y_prob.shape == (4,1), f\"Sortie attendue (4,1), obtenu {y_prob.shape}\"\n",
        "\n",
        "print(\"Device:\", device)\n",
        "print(\"Perte initiale:\", round(loss0, 6))\n",
        "print(\"Perte finale approx.:\", round(loss_last, 6))\n",
        "print(\"Probabilités:\", np.round(y_prob.reshape(-1), 4))\n",
        "print(\"Prédictions binaires:\", y_bin.reshape(-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RX6_VdHFeVO",
        "outputId": "daf760a4-89cb-4715-c147-f2eb1c6fad0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Perte initiale: 0.28656\n",
            "Perte finale approx.: 0.000561\n",
            "Probabilités: [0.0218 0.9737 0.9792 0.0254]\n",
            "Prédictions binaires: [0. 1. 1. 0.]\n"
          ]
        }
      ]
    }
  ]
}